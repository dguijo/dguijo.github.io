@conference{ReluVictorCAEPIA2021,
 abstract = {Activation functions are used in neural networks as a tool to introduce non-linear transformations into the model and, thus, enhance its representation capabilities. They also determine the output range of the hidden layers and the final output.  Traditionally, artificial neural networks mainly used the sigmoid activation function as the depth of the network was limited. Nevertheless, this function tends to saturate the gradients when the number of hidden layers increases. For that reason, in the last years, most of the works published related to deep learning and convolutional networks use the Rectified Linear Unit (ReLU), given that it provides good convergence properties and speeds up the training process thanks to the simplicity of its derivative. However, this function has some known drawbacks that gave rise to new proposals of alternatives activation functions based on ReLU. In this work, we describe, analyse and compare different recently proposed alternatives to test whether these functions improve the performance of deep learning models regarding the standard ReLU.},
 author = {Víctor Manuel Vargas-Yun and David Guijo-Rubio and Pedro Antonio Gutiérrez and César Hervás-Mart\ńez},
 booktitle = {Proceedings of the XIX Conference of the Spanish Association for Artificial Intelligence (CAEPIA)},
 editor = {Springer},
 keywords = {analysis activations, RELU, RELU activations, deep learning},
 month = {September},
 organization = {Malaga, Spain},
 series = { Lecture Notes in Artificial Intelligence (LNAI)},
 title = {ReLU-based activations: analysis and experimental study for deep learning},
 year = {2021}
}

